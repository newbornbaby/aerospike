# 1.简介
aerospike是高效的nosql数据库，采用纯内存或者内存+ssd方案存储数据，设计初衷达到了以下三个主要目的：
* 数据的快速、高效存取。
* 提供传统数据库的可靠性、健壮性、ACID能力。
* 灵活性、扩展性、容错能力。
  
# 2.架构设计
as采用**share nothing设计模式**，在各个处理单元中使用自己私有的cpu以及memory，不存在共享资源，类似于MMP（大规模并行处理）模式，各处理单元之间通过协议通信，并行处理和扩展能力强（典型的类似案例还有hadoop等），各节点处理完数据后，处理结果可以向上层汇总，或者在节点之间流转；此外，as的架构设计主要可以分为三层：
* **Client Layer（客户端）层**：实现as API，直接与集群通信、跟踪节点，发现节点的上下线、知道数据在节点中的位置。
* **Clustering and Data Distribution Layer（集群和数据分发）层**：管理集群、自动故障转移、高效分配任务、智能平衡数据、自动数据备份、数据迁移、跨数据中心同步数据。
* **Data Storage Layer（数据存储）层**：可靠地将数据存储到DRAM与FLASH中。（***as架构图待补充***）
                                
# 3.详细设计
## 3.1 Distribution Layer：
分发层旨在通过系统自动化所有集群管理功能，来最大程度的消除手动操作，该层有三个重要模块：
### 3.1.1 Cluster Managerment Module（集群管理模块）：
**重点**：通过Paxos-base gossip-voting process算法来确认集群内节点，使集群中所有节点对当前集群成员身份达成一致（Paxos是一中在分布式设计中广泛使用的分布式一致性协议），同时通过heartbeat监控各个节点的状态。
* **Cluster View（集群视图）**：集群中的每一个一个node都会注册一个identifier，有涉及mac以及port的一个算法产生；集群视图由<cluster_key, succession_list>标识，其中cluster_key是一个8位的随机数，succession_list是集群成员标识符集合，当集群成员状态发生变动时，会产生一个新的cluster_key以及对应的succession_list，来标识新视图；由于每次成员变动都会影响集群重新配置视图，导致处理延迟和处理效率下降，所以有必要使用一个一致性机制，快速有效地使集群重新达到一致状态。
* **Cluster Discovery**：集群中的每个node通过向其他节点周期性地持续地发送心跳包来告诉其他节点自己的状态，每个节点维护一份存活着的node列表，里面记录了会向这个节点发送消息的其他节点的标识，心跳时间吐过超过配置的超时时间还未收到，那么就移除该可能发生故障的节点。实际情况下，需要考虑到因网络抖动等问题，导致正常节点被排除的情况；此外，还需要避免一些不稳定节点频繁加入或者排除的情况。为了高效的使集群保持或者重新到达一致性状态，从以下两方面来考虑问题：*（1）Surrogate heartbeats（辅助心跳）*：除主心跳外的心跳机制，例如，将副本写入作为辅助心跳，若副本写入完好，或者主心跳完好，可确保仅主心跳上出现的网络异常，不会影响集群视图。*（2）Node Health Score（节点健康分数）*：衡量节点健康情况的分数，当节点的score分数超过所有节点平均值的两倍时，则说明该节点存在异常。_（***公式待补充***）_
* **Cluster View Change**：首先阐述一下集群view改变过程，当集群进入view更改周期（集群更改周期可以设置一般不宜设置太短，也不宜设置太长，太短会太频繁，太长会导致集群异常状态持续太长，请求无法处理。）时，将新增节点放在节点列表的最上面，作为paxos请求申请者，然后整个集群按照paxos算法（***算法的具体过程待补充***）来判断是否通过该请求，若通过则开始重新平衡数据，在请求正常通过的情况下，paoxs将花费三个网络周期时间，为了应对频繁修改view的情况，特设集群更改周期为超时时间的两倍，可以减少多次修改view状态带来的开销，多次修改可以在一个周期内完成。
* **Data Distribution**：一个primary key通过RipeMD160算法被变成160位的摘要，摘要被分配到4096个分区空间中，分区是Aerospike中最小的数据存在单位。根据主键摘要来为记录分配分区。即使键在键空间中的分布是倾斜的，但是摘要在分配到分区空间中时是均匀的，此数据分区方案是Aerospike独有的，有助于避免在数据访问期间创建热点，实现高级别的规模和容错。as在运行读取或者查询操作时，将索引与对应的记录放在同一个节点上可以避免数据的跨节点读取，索引与数据并置，在搭配一个健壮性高的分布式hash算法，可以达到各节点数据均价分布且一致性的效果。采用此种方式分配数据的好处：*（1）应用程序，工作负载在集群中均匀分布；（2）数据库操作性能是可预测的；（3）集群的向上和向下扩展很容易；（4）实时集群重新配置和随后的重新平衡是简单的、无中断的、高效的。*
* **A Partition Assignment Algorithm（分区分配算法）**：该算法为每个分区生成一个replication list，该列表的第一个节点是主节点，第二个节点是第一个副本节点，以此类推，最后可能有未使用的节点，分区分配的结果被称为partition map（***分区图的具体示例可以参照图1，待补充***）。当一个请求过来时，首先往主节点发送，当请求为read请求时，也可以均匀的通过所有的节点包括副本节点（这可以在请求时动态配置），一个as集群支持任何数量的拷贝，只要其拷贝数量不超过集群的节点数即可。分区分配算法需要达到以下几个目的：*（1）具有确定性，使分布式系统中每个节点均能单独地计算出相同的partition map；（2）实现主节点和副本节点在集群中的均匀分布；（3）在集群视图变动时，减小分区的变动。* 算法的核心NODE_HAS_COMPUTE这个方法将nodeId、partitionId结合生成一个hash值，REPLICATION_LIST_ASSIGN，该方法是用来获取replication list的，它将循环遍历succession_list，每次遍历都调用NODE_HAS_COMPUTE方法，将nodeId与hash值关联存储起来，然后按照nodeId的hash值排序。*（***算法的具体实现可以参照图2***）。到此，我们可以知道根据这个算法可以确定每个分区的节点分布图，且每个节点均有计算该分布图的能力，可以实现目的1；通过Jenkins one-at-a-time方法排列分区主副节点可以获取很好的散布性，可以实现目的2；由于分区图的确定，在增加或者减少节点时，可以尽可能的考虑重新分配分区，变动最小的方案，可以实现目的3。
