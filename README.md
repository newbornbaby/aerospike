# 1.简介
aerospike是高效的nosql数据库，采用纯内存或者内存+ssd方案存储数据，设计初衷达到了以下三个主要目的：
* 数据的快速、高效存取。
* 提供传统数据库的可靠性、健壮性、ACID能力。
* 灵活性、扩展性、容错能力。
  
# 2.架构设计
as采用**share nothing设计模式**，在各个处理单元中使用自己私有的cpu以及memory，不存在共享资源，类似于MMP（大规模并行处理）模式，各处理单元之间通过协议通信，并行处理和扩展能力强（典型的类似案例还有hadoop等），各节点处理完数据后，处理结果可以向上层汇总，或者在节点之间流转；此外，as的架构设计主要可以分为三层：
* **Client Layer（客户端）层**：实现as API，直接与集群通信、跟踪节点，发现节点的上下线、知道数据在节点中的位置。
* **Clustering and Data Distribution Layer（集群和数据分发）层**：管理集群、自动故障转移、高效分配任务、智能平衡数据、自动数据备份、数据迁移、跨数据中心同步数据。
* **Data Storage Layer（数据存储）层**：可靠地将数据存储到DRAM与FLASH中。（***as架构图待补充***）
                                
# 3.详细设计
## 3.1 Distribution Layer：
分发层旨在通过系统自动化所有集群管理功能，来最大程度的消除手动操作，该层有三个重要模块：
### 3.1.1 Cluster Managerment Module（集群管理模块）：
**重点**：通过Paxos-base gossip-voting process算法来确认集群内节点，使集群中所有节点对当前集群成员身份达成一致（Paxos是一中在分布式设计中广泛使用的分布式一致性协议），同时通过heartbeat监控各个节点的状态。
* **Cluster View（集群视图）**：集群中的每一个一个node都会注册一个identifier，有涉及mac以及port的一个算法产生；集群视图由<cluster_key, succession_list>标识，其中cluster_key是一个8位的随机数，succession_list是集群成员标识符集合，当集群成员状态发生变动时，会产生一个新的cluster_key以及对应的succession_list，来标识新视图；由于每次成员变动都会影响集群重新配置视图，导致处理延迟和处理效率下降，所以有必要使用一个一致性机制，快速有效地使集群重新达到一致状态。
* **Cluster Discovery**：集群中的每个node通过向其他节点周期性地持续地发送心跳包来告诉其他节点自己的状态，每个节点维护一份存活着的node列表，里面记录了会向这个节点发送消息的其他节点的标识，心跳时间吐过超过配置的超时时间还未收到，那么就移除该可能发生故障的节点。实际情况下，需要考虑到因网络抖动等问题，导致正常节点被排除的情况；此外，还需要避免一些不稳定节点频繁加入或者排除的情况。为了高效的使集群保持或者重新到达一致性状态，从以下两方面来考虑问题：*（1）Surrogate heartbeats（辅助心跳）*：除主心跳外的心跳机制，例如，将副本写入作为辅助心跳，若副本写入完好，或者主心跳完好，可确保仅主心跳上出现的网络异常，不会影响集群视图。*（2）Node Health Score（节点健康分数）*：衡量节点健康情况的分数，当节点的score分数超过所有节点平均值的两倍时，则说明该节点存在异常。_（***公式待补充***）_
* **Cluster View Change**：首先阐述一下集群view改变过程，当集群进入view更改周期（集群更改周期可以设置一般不宜设置太短，也不宜设置太长，太短会太频繁，太长会导致集群异常状态持续太长，请求无法处理。）时，将新增节点放在节点列表的最上面，作为paxos请求申请者，然后整个集群按照paxos算法（***算法的具体过程待补充***）来判断是否通过该请求，若通过则开始重新平衡数据，在请求正常通过的情况下，paoxs将花费三个网络周期时间，为了应对频繁修改view的情况，特设集群更改周期为超时时间的两倍，可以减少多次修改view状态带来的开销，多次修改可以在一个周期内完成。
* **Data Distribution**：一个primary key通过RipeMD160算法被变成160位的摘要，摘要被分配到4096个分区空间中，分区是Aerospike中最小的数据存在单位。根据主键摘要来为记录分配分区。即使键在键空间中的分布是倾斜的，但是摘要在分配到分区空间中时是均匀的，此数据分区方案是Aerospike独有的，有助于避免在数据访问期间创建热点，实现高级别的规模和容错。as在运行读取或者查询操作时，将索引与对应的记录放在同一个节点上可以避免数据的跨节点读取，索引与数据并置，在搭配一个健壮性高的分布式hash算法，可以达到各节点数据均价分布且一致性的效果。采用此种方式分配数据的好处：*（1）应用程序，工作负载在集群中均匀分布；（2）数据库操作性能是可预测的；（3）集群的向上和向下扩展很容易；（4）实时集群重新配置和随后的重新平衡是简单的、无中断的、高效的。*
* **A Partition Assignment Algorithm（分区分配算法）**：该算法为每个分区生成一个replication list，该列表的第一个节点是主节点，第二个节点是第一个副本节点，以此类推，最后可能有未使用的节点，分区分配的结果被称为partition map（***分区图的具体示例可以参照图1，待补充***）。当一个请求过来时，首先往主节点发送，当请求为read请求时，也可以均匀的通过所有的节点包括副本节点（这可以在请求时动态配置），一个as集群支持任何数量的拷贝，只要其拷贝数量不超过集群的节点数即可。分区分配算法需要达到以下几个目的：*（1）具有确定性，使分布式系统中每个节点均能单独地计算出相同的partition map；（2）实现主节点和副本节点在集群中的均匀分布；（3）在集群视图变动时，减小分区的变动。* 算法的核心NODE_HAS_COMPUTE这个方法将nodeId、partitionId结合生成一个hash值，REPLICATION_LIST_ASSIGN，该方法是用来获取replication list的，它将循环遍历succession_list，每次遍历都调用NODE_HAS_COMPUTE方法，将nodeId与hash值关联存储起来，然后按照nodeId的hash值排序。*（***算法的具体实现可以参照图2***）。到此，我们可以知道根据这个算法可以确定每个分区的节点分布图，且每个节点均有计算该分布图的能力，可以实现目的1；通过Jenkins one-at-a-time方法排列分区主副节点可以获取很好的散布性，可以实现目的2；由于分区图的确定，在增加或者减少节点时，可以尽可能的考虑重新分配分区，变动最小的方案，可以实现目的3。
### 3.1.2 Date Migration Module（数据迁移模块）
**重点概述**：将记录从一个节点迁移到另一个节点的过程称为迁移。每次集群视图更改后，数据迁移开始，其目标是在每个数据分区的当前主节点和副本节点上提供每个记录的最新版本。一旦在新的视图上达成共识，集群中的所有节点运行分区分配算法，并将主节点和一个或多个副本节点分配给每个分区。每个分区的主节点为该分区分配一个唯一的分区版本，此版本号将复制到副本节点。在集群视图更改之后，节点之间交换每个具有数据的分区的分区版本。因此，每个主节点都知道分区的每个副本的版本号。接下来详细描述节点挂了之后的数据迁移过程：当一个节点挂了之后，其心跳停止，其他节点在集群视图更改周期中发起移除申请，根据paxos算法，集群达成移除共识，集群的视图发生改变，seccession_list更新，各个节点运行分区分配算法，计算出新的统一分区分布图，分区重新分布，新分区中当前主节点和副本节点之间交换每个记录的最新版本，交换完成后主节点知道其他每个分区的版本号，它为分区生成一个统一的新的分区版本号，之后将该分区版本号传递给其他副本节点达成一致状态，副本数据分区可以通过copy的方式从当前主分区（可能是代理主分区，之后讨论何为代理主分区，在这里你就认为是一个临时主分区）获取数据，为了减少工作量，对比副本节点数据区数据和当前主节点数据区数据，只需要将不同的数据采用更新策略即可，一样的数据可不做处理，在所有分区的主副节点的数据区copy完成后，数据迁移结束。
* **Delta-Migrations（增量迁移）**：为了缩短数据迁移的时间，减少集群开销，as采取了一些优化策略来提高效率。在数据迁移的过程中，as会尝试先为每个分区的当前版本进行排序，对于同一分区，当node1的分区版本小于node2的分区版本时，显然node1的数据需要从当前主节点重新同步，当node2的版本恰好和当前分区版本一致时，node2就不需要同步了，节省了集群开销。但是，并不一定能够为所有的分区的版本都能被排序，当集群节点之间有网络分区时，有些节点无法参与排序，此时，集群中的一些节点可能无法保证此时该记录是集群的最新版本。
* **Operations During Migrations（数据迁移时的请求处理）**：在数据迁移期间，如果发生read请求，as保证返回最终获取胜利的记录，即迁移后的最终结果数据。对于部分的写操作，as保证记录将写入最终获取胜利的记录，即迁移后的最终结果数据。为了确保上述承诺的实现，在数据迁移期间，as会让操作进入duplicate resolution（重复判断）逻辑，在duplicate resolution期间，包含特定记录的分区当前主节点将读取其所有分区版本中的记录，并解析为该记录的一个副本（最新）。这是获胜的副本，今后将用于读或写事务。
* **Master Partition Without Data**：在集群视图变动时，可能出现部分分区新的副本列表主节点为空的情况，这时当读写请求进来后，进入duplicate resolution（重复判断）期间，在此期间时，会选取一个记录最多的节点当代理主节点，代理节点在迁移完成后结束生命周期，节点在同步未完成时会有一个DESYNC标志（未同步），当然如果请求主动声明不需要进入重复判断阶段，能够接受可能出现的返回数据不全或者数据过期或者空数据的情况，也可以。
* **Migration Ordering（数据迁移顺序涉及两个优化策略Smallest Partition First、Hottest Partition First）**：在数据迁移期间，as没有设定迁移操作的优先级以及其他操作（例如读取操作）的优先级谁高谁低，所以基本上这些操作会同时进行，为了减少在数据迁移期间处理其他操作带来的额外集群开销，需要尽可能快的完成数据迁移。as有以下两种策略来优化性能：***（1）Smallest Partition First***：最小分区优先迁移原则；***（2）Hottest Partition First***：热点数据优先迁移原则。
* **Summary**：数据的统一分布、相关的元数据（如索引）和事务工作负载使Aerospike集群的容量规划和上下扩展决策精确而简单。Aerospike只需要对集群成员的变化进行数据重新分配。这与基于备用键范围的分区方案形成了对比，后者要求在范围“大于”其节点上的容量时重新分配数据。

### 3.1.3 Transaction Processing Module（事务操作模块）
**重点概述**：读写数据时，as提供一致性和隔离性保证，此外，该模块提供以下保证。
* **Sync/Async Replication**：对于具有即时一致性写入，在返回和提交数据之前，将广播所有改变同步到所有备份节点。
* **Proxy**：在一些极少数的情况下，当集群重新配置时，客户端层可能暂时过时，事务处理模块可以透明地将请求代理到另一个节点执行。
* **Duplicate Replication**：当集群从分区中恢复时，需要解决不同数据副本之间的任何可能出现的冲突，该模块即可提供此功能，当一个集群启动后，可以在其他数据中心安装其他集群，并设置跨数据中心恢复，以确保如果数据中心关闭，远程集群将以最小或不中断用户请求的方式接管工作。
